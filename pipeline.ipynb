{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK7Z5OuNJvls"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "from librosa import load\n",
        "from librosa.util import normalize\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_8nFv9_3IZB"
      },
      "outputs": [],
      "source": [
        "#@title Dataset\n",
        "#Определяем класс, позволяющий получать примеры реальных композиций в нужном формате\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "DURATION = 60\n",
        "AUDIO_SHAPE = SAMPLE_RATE*DURATION\n",
        "\n",
        "loops_simult = 5\n",
        "loops_seq = 8\n",
        "\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset_path, dataset_size, segment_length, sampling_rate):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.segment_length = segment_length\n",
        "        self.datase_path = dataset_path\n",
        "        self.audio_files = os.listdir(dataset_path)[:dataset_size]\n",
        "        self.dataset_size = dataset_size\n",
        "        random.seed(1234)\n",
        "        random.shuffle(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Read audio\n",
        "        filename = self.audio_files[index]\n",
        "        audio, sampling_rate = self.load_wav_to_torch(self.datase_path + filename)\n",
        "        # Take segment\n",
        "        if audio.size(0) >= self.segment_length:\n",
        "            max_audio_start = audio.size(0) - self.segment_length\n",
        "            audio_start = random.randint(0, max_audio_start)\n",
        "            audio = audio[audio_start : audio_start + self.segment_length]\n",
        "        else:\n",
        "            audio = F.pad(\n",
        "                audio, (0, self.segment_length - audio.size(0)), \"constant\"\n",
        "            ).data\n",
        "\n",
        "        # audio = audio / 32768.0\n",
        "        return audio.unsqueeze(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def load_wav_to_torch(self, full_path):\n",
        "        \"\"\"\n",
        "        Loads wavdata into torch array\n",
        "        \"\"\"\n",
        "        data, sampling_rate = load(full_path, sr=self.sampling_rate)\n",
        "        data = 0.95 * normalize(data)\n",
        "\n",
        "        return torch.from_numpy(data).float(), sampling_rate\n",
        "\n",
        "DATASET_PATH = '...' # Указать путь к папке, содержащей примеры реальной музыки\n",
        "\n",
        "train_set = AudioDataset(DATASET_PATH, 128, AUDIO_SHAPE, SAMPLE_RATE)\n",
        "val_set = AudioDataset(DATASET_PATH, 32, AUDIO_SHAPE, SAMPLE_RATE)\n",
        "\n",
        "batch_size = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ2I32Ww_WGS"
      },
      "outputs": [],
      "source": [
        "# Сохраняем лупы в массив\n",
        "\n",
        "LOOPS_PATH = '...' # Указать путь к папке, содержащей лупы\n",
        "\n",
        "loops = []\n",
        "\n",
        "loops_dict = dict()\n",
        "for i, fname in enumerate(os.listdir(LOOPS_PATH)):\n",
        "  loops_dict[i+1] = fname\n",
        "\n",
        "num_loops = len(loops_dict)\n",
        "\n",
        "for fname in os.listdir(LOOPS_PATH):\n",
        "  audio, sr = load(LOOPS_PATH + fname)\n",
        "  loops.append(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrCegp66g3pw"
      },
      "outputs": [],
      "source": [
        "#@title Loops2Audio function\n",
        "\n",
        "# Определяем функцию, преобразующую матрицу лупов в аудио сигнал\n",
        "\n",
        "def get_audio_from_loops(input):\n",
        "\n",
        "  input = input.detach().numpy()\n",
        "\n",
        "  batch_size = input.shape[0]\n",
        "\n",
        "  songs = np.empty([batch_size, 1, AUDIO_SHAPE])\n",
        "\n",
        "  for i, ids_to_play in enumerate(input):\n",
        "\n",
        "    ids_to_play = np.reshape(ids_to_play, (loops_seq, loops_simult))\n",
        "    song = np.array([])\n",
        "    needed_size = AUDIO_SHAPE // len(ids_to_play)\n",
        "\n",
        "    for ids_takt in ids_to_play:\n",
        "      audio_takt = np.zeros(needed_size)\n",
        "\n",
        "      for id in ids_takt:\n",
        "        if id == 0:\n",
        "          continue\n",
        "        audio = loops[int(id)-1]\n",
        "\n",
        "        actual_size = audio.shape[0]\n",
        "\n",
        "        if actual_size == needed_size:\n",
        "          audio_takt += audio\n",
        "\n",
        "        elif actual_size > needed_size:\n",
        "          audio_takt += audio[:needed_size]\n",
        "\n",
        "        else:\n",
        "          alpha = needed_size // actual_size\n",
        "          audio_stacked = np.array([])\n",
        "          for i in range(alpha):\n",
        "            audio_stacked = np.hstack([audio_stacked, audio])\n",
        "          audio_takt += audio_stacked\n",
        "\n",
        "      song = np.hstack([song, np.array(audio_takt)])\n",
        "\n",
        "    songs[i] = song\n",
        "\n",
        "  return torch.from_numpy(songs).to(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gge2Mwq-0Lwx"
      },
      "outputs": [],
      "source": [
        "#@title Utilites\n",
        "\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "# Инициализация весов для нейросетевых моделей\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def WNConv1d(*args, **kwargs):\n",
        "    return weight_norm(nn.Conv1d(*args, **kwargs))\n",
        "\n",
        "\n",
        "def WNConvTranspose1d(*args, **kwargs):\n",
        "    return weight_norm(nn.ConvTranspose1d(*args, **kwargs))\n",
        "\n",
        "# Определяем архитектуру остаточного (ResNet) блока\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dilation=1):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ReflectionPad1d(dilation),\n",
        "            WNConv1d(dim, dim, kernel_size=3, dilation=dilation),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            WNConv1d(dim, dim, kernel_size=1),\n",
        "        )\n",
        "        self.shortcut = WNConv1d(dim, dim, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.shortcut(x) + self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD1TulQOHcg7"
      },
      "outputs": [],
      "source": [
        "#@title Generator\n",
        "# Определяем архитектуру генератора\n",
        "\n",
        "nz = 100\n",
        "ngf = 64\n",
        "nc = loops_simult\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose1d(nz, ngf * 2, 2, 2),\n",
        "            nn.BatchNorm1d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose1d(ngf * 2, ngf, 2, 2),\n",
        "            nn.BatchNorm1d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose1d(ngf, nc, 2, 2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        output = torch.sigmoid(output)*num_loops\n",
        "\n",
        "        rounded = torch.round(output)\n",
        "\n",
        "        rounded = output + (rounded - output).detach()\n",
        "\n",
        "        return rounded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOKbk1vrPhxv"
      },
      "outputs": [],
      "source": [
        "#@title Loops2Audio predictor\n",
        "# Определяем архитектуру аппроксиматора\n",
        "ngf2 = 4\n",
        "\n",
        "class Loops2Audio(nn.Module):\n",
        "  def __init__(self, input_size=nc, n_residual_layers=3):\n",
        "        super().__init__()\n",
        "        ratios = [8, 8, 5, 5, 5, 5, 3]\n",
        "        self.hop_length = np.prod(ratios)\n",
        "        mult = int(2 ** len(ratios))\n",
        "\n",
        "        model = [\n",
        "            nn.ReflectionPad1d(3),\n",
        "            WNConv1d(input_size, mult * ngf2, kernel_size=7, padding=0),\n",
        "        ]\n",
        "\n",
        "        # Upsample to raw audio scale\n",
        "        for i, r in enumerate(ratios):\n",
        "            model += [\n",
        "                nn.LeakyReLU(0.2),\n",
        "                WNConvTranspose1d(\n",
        "                    mult * ngf2,\n",
        "                    mult * ngf2 // 2,\n",
        "                    kernel_size=r * 2,\n",
        "                    stride=r,\n",
        "                    padding=r // 2 + r % 2,\n",
        "                    output_padding=r % 2,\n",
        "                ),\n",
        "            ]\n",
        "\n",
        "            for j in range(n_residual_layers):\n",
        "                model += [ResnetBlock(mult * ngf2 // 2, dilation=3 ** j)]\n",
        "\n",
        "            mult //= 2\n",
        "\n",
        "        model += [\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ReflectionPad1d(3),\n",
        "            WNConv1d(ngf2, 1, kernel_size=7, padding=0),\n",
        "            nn.Tanh(),\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "        self.apply(weights_init)\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjH2GsqANYMU"
      },
      "outputs": [],
      "source": [
        "#@title CNN Discriminator\n",
        "# Определяем архитектуру дискриминатора\n",
        "\n",
        "ndf = 16\n",
        "n_layers = 4\n",
        "downsampling_factor = 4\n",
        "\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "def WNConv1d(*args, **kwargs):\n",
        "    return weight_norm(nn.Conv1d(*args, **kwargs))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        model = nn.ModuleDict()\n",
        "\n",
        "        model[\"layer_0\"] = nn.Sequential(\n",
        "            nn.ReflectionPad1d(7),\n",
        "            WNConv1d(1, ndf, kernel_size=15),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        nf = ndf\n",
        "        stride = downsampling_factor\n",
        "        for n in range(1, n_layers + 1):\n",
        "            nf_prev = nf\n",
        "            nf = min(nf * stride, 1024)\n",
        "\n",
        "            model[\"layer_%d\" % n] = nn.Sequential(\n",
        "                WNConv1d(\n",
        "                    nf_prev,\n",
        "                    nf,\n",
        "                    kernel_size=stride * 10 + 1,\n",
        "                    stride=stride,\n",
        "                    padding=stride * 5,\n",
        "                    groups=nf_prev // 4,\n",
        "                ),\n",
        "                nn.LeakyReLU(0.2, True),\n",
        "            )\n",
        "\n",
        "        nf = min(nf * 2, 1024)\n",
        "        model[\"layer_%d\" % (n_layers + 1)] = nn.Sequential(\n",
        "            WNConv1d(nf_prev, nf, kernel_size=5, stride=1, padding=2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        model[\"layer_%d\" % (n_layers + 2)] = WNConv1d(\n",
        "            nf, 1, kernel_size=3, stride=1, padding=1\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        results = []\n",
        "        for key, layer in self.model.items():\n",
        "            x = layer(x)\n",
        "            results.append(x)\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUFs6fRs3K1v",
        "outputId": "761f25c0-0eb8-4aae-e353-fb32a90c455a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        }
      ],
      "source": [
        "netG = Generator().cuda()\n",
        "netL2A = Loops2Audio().cuda()\n",
        "netD = Discriminator().cuda()\n",
        "\n",
        "optG = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optL2A = torch.optim.Adam(netL2A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optD = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "gan_loss = nn.BCELoss()\n",
        "loss_fn = F.mse_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-KHE6cT59B-"
      },
      "outputs": [],
      "source": [
        "#@title Training Loops2Audio model\n",
        "# Обучение аппроксиматора\n",
        "\n",
        "l2a_errs = []\n",
        "l2a_epochs = 1000\n",
        "\n",
        "for epoch in range(1, l2a_epochs + 1):\n",
        "\n",
        "    rand_ids = torch.randint(low=0, high=27, size=(5, 8)).unsqueeze(dim=0).cuda()\n",
        "\n",
        "    audio_pred = netL2A(rand_ids.to(torch.float32))\n",
        "\n",
        "    audio_g = get_audio_from_loops(rand_ids.cpu()).cuda()\n",
        "\n",
        "    l2a_error = loss_fn(audio_pred, audio_g, reduction='sum')\n",
        "\n",
        "    netL2A.zero_grad()\n",
        "    l2a_error.backward()\n",
        "    optL2A.step()\n",
        "\n",
        "    del audio_pred\n",
        "    del audio_g\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    l2a_errs.append(l2a_error.item())\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      print(\"epoch num {} loss_L2A = {}\".format(epoch, l2a_error.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qowczyv-UcrW"
      },
      "outputs": [],
      "source": [
        "#@title Training\n",
        "# Обучение GAN\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "train_losses_D = []\n",
        "train_losses_G = []\n",
        "val_losses_D = []\n",
        "val_losses_G = []\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "\n",
        "    for iterno, audio_r in enumerate(train_loader):\n",
        "\n",
        "        audio_r = audio_r.cuda().to(torch.float32)\n",
        "\n",
        "        noise = torch.randn(batch_size, nz, 1).cuda()\n",
        "        ids = netG(noise)\n",
        "\n",
        "        #######################\n",
        "        # Train Discriminator #\n",
        "        #######################\n",
        "        D_fake_det = netD(audio_pred.cuda().detach())\n",
        "        D_real = netD(audio_r.cuda())\n",
        "\n",
        "        loss_D = 0\n",
        "        for scale in D_fake_det:\n",
        "            loss_D += F.relu(1 + scale[-1]).mean()\n",
        "\n",
        "        for scale in D_real:\n",
        "            loss_D += F.relu(1 - scale[-1]).mean()\n",
        "\n",
        "        netD.zero_grad()\n",
        "        loss_D.backward()\n",
        "        optD.step()\n",
        "\n",
        "        ###################\n",
        "        # Train Generator #\n",
        "        ###################\n",
        "        audio_pred = netL2A(ids)\n",
        "        D_fake = netD(audio_pred.cuda())\n",
        "\n",
        "        loss_G = 0\n",
        "        for scale in D_fake:\n",
        "            loss_G += -scale[-1].mean()\n",
        "\n",
        "        loss_feat = 0\n",
        "        feat_weights = 4.0 / (n_layers + 1)\n",
        "        D_weights = 1.0\n",
        "        wt = D_weights * feat_weights\n",
        "        for i in range(1):\n",
        "            for j in range(len(D_fake[i]) - 1):\n",
        "                loss_feat += wt * F.l1_loss(D_fake[i][j], D_real[i][j].detach())\n",
        "\n",
        "        netG.zero_grad()\n",
        "        (loss_G + 10 * loss_feat).backward()\n",
        "        optG.step()\n",
        "\n",
        "        del audio_r\n",
        "        del audio_pred\n",
        "        del audio_g\n",
        "        del noise\n",
        "        del D_fake_det\n",
        "        del D_real\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(\"epoch num {} loss_G = {}, loss_D = {}\".format(epoch, loss_G, loss_D))\n",
        "\n",
        "        train_losses_G.append(loss_G)\n",
        "        train_losses_D.append(loss_D)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохраняем параметры обученного генератора\n",
        "\n",
        "torch.save(netG.state_dict(), '...') # Указать путь к .pth файлу, в который требуется сохранить параметры генератора"
      ],
      "metadata": {
        "id": "sZ_EgtxGTHXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGjxFyqdmm7V"
      },
      "outputs": [],
      "source": [
        "# Загружаем параметры предобученного генератора\n",
        "\n",
        "netG = Generator().cuda()\n",
        "netG.load_state_dict(torch.load('...')) # Указать путь к .pth файлу, содержащему параметры предобученного генератора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OCiuEHLj6DU"
      },
      "outputs": [],
      "source": [
        "# Генерируем музыку\n",
        "\n",
        "noise = torch.randn(1, nz, 1).cuda()\n",
        "ids = netG(noise)\n",
        "audio_g = get_audio_from_loops(ids.cpu()).cuda()\n",
        "audio_pred = netL2A(ids.detach()).detach()\n",
        "ipd.Audio(audio_g.squeeze().cpu(), rate=SAMPLE_RATE)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
